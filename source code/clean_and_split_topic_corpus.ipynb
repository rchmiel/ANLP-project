{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------\n",
    "#This code reads a .json file with Twitter data, cleans it and\n",
    "#writes cleaned data into a file\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "import ijson\n",
    "from nltk.corpus import words\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#Function for splitting sentences\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    caps = \"([A-Z])\"\n",
    "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "    starters = \"(Mr|Rev|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\\n\",\" \")\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"amp&;\", \"&\")\n",
    "    text = text.replace(\"&amp;\", \"&\")\n",
    "    text = text.replace(\"p.m.\", \"P.M.\")\n",
    "    text = text.replace(\"a.m.\", \"A.M.\")\n",
    "    text = re.sub(r\"http.*\", \"\", text)\n",
    "    text = re.sub(r\"www.*\", \"\", text)\n",
    "    text = re.sub(r\"#.*\", \"\", text) \n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    #print(\"3 - \" + text)\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#Make .json files readable for ijson\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "path = '..\\\\data\\\\'\n",
    "input_filename = 'twitter_stream_example.json'\n",
    "output_filename = 'twitter_stream_example_edited.json'\n",
    "final_topic_corpus_name = output_filename + '.txt'\n",
    "\n",
    "input_json_file = path + input_filename\n",
    "output_json_file = path + output_filename\n",
    "\n",
    "output = []\n",
    "\n",
    "with open(input_json_file) as infile, open(output_json_file, 'w') as outfile:\n",
    "    for idx, line in enumerate(infile):\n",
    "        if len(line) < 100:\n",
    "            pass\n",
    "        else:\n",
    "            if idx == 0:\n",
    "                output.append('[' + line.strip() + ',')\n",
    "            else:\n",
    "                output.append(line.strip() + ',')\n",
    "                \n",
    "    output[len(output)-1] = output[len(output)-1][:len(output[len(output)-1])-1] + ']' #remove last , and add ]\n",
    "\n",
    "    for item in output:\n",
    "        print(item, file=outfile)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "#Reading a .json file and work with the data with ijson\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "topic_corpus_file = output_json_file\n",
    "\n",
    "with open(topic_corpus_file, 'r') as f:\n",
    "    objects = ijson.items(f, 'item')\n",
    "    columns = list(objects)\n",
    "\n",
    "raw_topic_tweets=[]\n",
    "for col in columns:\n",
    "    if col[\"user\"][\"friends_count\"]>300 and col[\"user\"][\"friends_count\"]<1500 and col[\"user\"][\"followers_count\"]>300:\n",
    "        try:\n",
    "            raw_topic_tweets.append(col[\"extended_tweet\"][\"full_text\"]) #full text tweet of all tweets that are no RT\n",
    "        except KeyError:\n",
    "            continue\n",
    "            \n",
    "#---------------------------------------------------------------------\n",
    "#Clean topic corpus and split into sentences\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "topic_sentences = []\n",
    "raw_topic_corpus = []\n",
    "\n",
    "#create list of sentences out of tweets\n",
    "for tweet in raw_topic_tweets:\n",
    "    if 'deal' not in str(tweet) and '24' not in str(tweet) and 'order' not in str(tweet) and 'price' not in str(tweet) and '$' not in str(tweet) and 'sale' not in str(tweet): topic_sentences.append(split_into_sentences(tweet))\n",
    "    \n",
    "#flatten list of topic sentences\n",
    "flat_topic_corpus = [item for sublist in topic_sentences for item in sublist]\n",
    "\n",
    "#handle the multiple use of \".\", \"?\" and \"!\" and p.m. a.m.\n",
    "numbers = re.compile(\"[0-9]\")\n",
    "number_plus_dot = re.compile(\"[0-9]\\.\") \n",
    "\n",
    "#clean output\n",
    "for index, sentence in enumerate(flat_topic_corpus):\n",
    "    if sentence == \".\": #add separated multiple dots back to the sentence\n",
    "        if raw_topic_corpus[-1][-1]== \".\":\n",
    "            raw_topic_corpus[-1]=raw_topic_corpus[-1]+\".\"\n",
    "    elif sentence ==\"!\":  #add separated multiple \"!\" back to the sentence\n",
    "        raw_topic_corpus[-1]=raw_topic_corpus[-1]+\"!\"\n",
    "    elif sentence ==\"?\":  #add separated multiple \"?\" back to the sentence\n",
    "        raw_topic_corpus[-1]=raw_topic_corpus[-1]+\"?\"\n",
    "    elif numbers.match(sentence[0]): #if sentence starts with number and the last sentence ends with a number and dot then merge\n",
    "        if number_plus_dot.match(raw_topic_corpus[-1][-2:]):\n",
    "            raw_topic_corpus[-1] = raw_topic_corpus[-1] + sentence\n",
    "    else:\n",
    "        raw_topic_corpus.append(sentence)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "#Write topic corpus to file\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "remove_url = re.compile(\"https:*\")\n",
    "file = open('..\\\\data\\\\' + final_topic_corpus_name, 'w', encoding = 'utf-8') \n",
    "    \n",
    "for sentence in raw_topic_corpus:\n",
    "    check_latin = True\n",
    "    try: \n",
    "        sentence.encode('ascii')\n",
    "    except UnicodeEncodeError: \n",
    "        check_latin = False\n",
    "        \n",
    "    if check_latin == True: #only use sentences written in latin\n",
    "        if len(sentence) > 20: #clean short spamlike tweets\n",
    "            for word in sentence.split():\n",
    "                if remove_url.match(word): #remove links\n",
    "                    continue\n",
    "                elif word == '&amp;': #correct '&' symbols\n",
    "                    file.write(\"& \")\n",
    "                elif word[:1] == '@': #remove @persons\n",
    "                    file.write(\"\")\n",
    "                else:\n",
    "                    #EMOJI-FIX\n",
    "                    try: #remove emojiis\n",
    "                        file.write(word + \" \")\n",
    "                    except UnicodeEncodeError:\n",
    "                        file.write(\"\")\n",
    "            file.write('\\n')\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
