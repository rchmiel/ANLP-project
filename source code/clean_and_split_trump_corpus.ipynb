{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------\n",
    "#This code cleans and splits the Trump corpus\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "import re\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#Function for splitting sentences\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "def split_into_sentences(text):\n",
    "    caps = \"([A-Z])\"\n",
    "    prefixes = \"(Mr|St|Mrs|Ms|Dr)[.]\"\n",
    "    suffixes = \"(Inc|Ltd|Jr|Sr|Co)\"\n",
    "    starters = \"(Mr|Rev|Mrs|Ms|Dr|He\\s|She\\s|It\\s|They\\s|Their\\s|Our\\s|We\\s|But\\s|However\\s|That\\s|This\\s|Wherever)\"\n",
    "    acronyms = \"([A-Z][.][A-Z][.](?:[A-Z][.])?)\"\n",
    "    websites = \"[.](com|net|org|io|gov)\"\n",
    "    text = \" \" + text + \"  \"\n",
    "    text = text.replace(\"\\n\\n\",\" \")\n",
    "    text = text.replace(\"\\n\",\" \")\n",
    "    text = text.replace(\"amp&;\", \"&\")\n",
    "    text = text.replace(\"&amp;\", \"&\")\n",
    "    text = text.replace(\"p.m.\", \"P.M.\")\n",
    "    text = text.replace(\"a.m.\", \"A.M.\")\n",
    "    text = re.sub(r\"http.*\", \"\", text)\n",
    "    text = re.sub(r\"www.*\", \"\", text)\n",
    "    text = re.sub(r\"#.*\", \"\", text) \n",
    "    text = re.sub(prefixes,\"\\\\1<prd>\",text)\n",
    "    text = re.sub(websites,\"<prd>\\\\1\",text)\n",
    "    if \"Ph.D\" in text: text = text.replace(\"Ph.D.\",\"Ph<prd>D<prd>\")\n",
    "    text = re.sub(\"\\s\" + caps + \"[.] \",\" \\\\1<prd> \",text)\n",
    "    text = re.sub(acronyms+\" \"+starters,\"\\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\\\\3<prd>\",text)\n",
    "    text = re.sub(caps + \"[.]\" + caps + \"[.]\",\"\\\\1<prd>\\\\2<prd>\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.] \"+starters,\" \\\\1<stop> \\\\2\",text)\n",
    "    text = re.sub(\" \"+suffixes+\"[.]\",\" \\\\1<prd>\",text)\n",
    "    text = re.sub(\" \" + caps + \"[.]\",\" \\\\1<prd>\",text)\n",
    "    if \"”\" in text: text = text.replace(\".”\",\"”.\")\n",
    "    if \"\\\"\" in text: text = text.replace(\".\\\"\",\"\\\".\")\n",
    "    if \"!\" in text: text = text.replace(\"!\\\"\",\"\\\"!\")\n",
    "    if \"?\" in text: text = text.replace(\"?\\\"\",\"\\\"?\")\n",
    "    text = text.replace(\".\",\".<stop>\")\n",
    "    text = text.replace(\"?\",\"?<stop>\")\n",
    "    text = text.replace(\"!\",\"!<stop>\")\n",
    "    text = text.replace(\"<prd>\",\".\")\n",
    "    #print(\"3 - \" + text)\n",
    "    sentences = text.split(\"<stop>\")\n",
    "    sentences = sentences[:-1]\n",
    "    sentences = [s.strip() for s in sentences]\n",
    "    return sentences\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#Clean the Trump corpus\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "trump_corpus_file = open('..\\\\data\\\\trump_corpus_14012018.txt', encoding = 'utf-8')\n",
    "trump_corpus = trump_corpus_file.read()\n",
    "\n",
    "trump_list = eval(trump_corpus) #list of dicts\n",
    "\n",
    "trump_tweets = [] #from newest to oldest tweet\n",
    "rev_trump_tweets = [] #from oldest to newest tweet\n",
    "cleaned_trump_corpus = []\n",
    "trump_sentences = []\n",
    "final_trump_corpus = []\n",
    "\n",
    "#create list of Trump tweets\n",
    "for tweet in trump_list[:-2000]: #the first 2000 tweets mostly not by Trump himself\n",
    "    \n",
    "    #remove retweets & create list of tweets\n",
    "    if tweet['text'][0:2] != \"RT\" and tweet['text'][0:2] != \"RE\" and tweet['text'][0]!=\"@\":\n",
    "        trump_tweets.append(tweet['text'])\n",
    "\n",
    "#merges ... separated tweets\n",
    "for tweet in reversed(trump_tweets):\n",
    "    rev_trump_tweets.append(tweet)\n",
    "\n",
    "for index, tweet in enumerate(rev_trump_tweets):\n",
    "    if tweet[-3:] == \"...\":\n",
    "        if rev_trump_tweets[index+1][0:3] == \"...\":\n",
    "\n",
    "            dot_counter = 0 #counts the number of tweet separation dots\n",
    "\n",
    "            for letter in reversed(tweet):\n",
    "                if letter == \".\":\n",
    "                    dot_counter += 1\n",
    "                else:\n",
    "                    break\n",
    "\n",
    "            rev_trump_tweets[index+1] = tweet[0:len(tweet)-dot_counter] + ' ' + rev_trump_tweets[index+1][dot_counter:]\n",
    "            rev_trump_tweets[index] = \"&DELETE&\" #mark for later deletion\n",
    "\n",
    "#remove the marked tweets and clean the tweets\n",
    "for tweet in rev_trump_tweets:\n",
    "    if tweet != \"&DELETE&\" and tweet[:2]!='“@' and tweet[:2]!='\"@' and tweet[:3]!='Via' and tweet[:1]!='“' and tweet[:1]!='\"' and tweet[:1]!=\"@\": #remove, marked tweets & things trump replied to or posted (because mostly just \"thanks\")\n",
    "        cleaned_trump_corpus.append(tweet)\n",
    "        \n",
    "#---------------------------------------------------------------------\n",
    "#Split into sentences\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "#create list of Trump sentences out of tweets\n",
    "for tweet in cleaned_trump_corpus:\n",
    "    trump_sentences.append(split_into_sentences(tweet))\n",
    "\n",
    "#flatten list of Trump sentences\n",
    "flat_trump_corpus = [item for sublist in trump_sentences for item in sublist]\n",
    "\n",
    "#handle the multiple use of \".\", \"?\" and \"!\" and p.m. a.m.\n",
    "numbers = re.compile(\"[0-9]\")\n",
    "number_plus_dot = re.compile(\"[0-9]\\.\") \n",
    "\n",
    "#clean output\n",
    "for index, sentence in enumerate(flat_trump_corpus):\n",
    "    if sentence == \".\": #add separated multiple dots back to the sentence\n",
    "        if final_trump_corpus[-1][-1]== \".\":\n",
    "            final_trump_corpus[-1]=final_trump_corpus[-1]+\".\"\n",
    "    elif sentence ==\"!\":  #add separated multiple \"!\" back to the sentence\n",
    "        final_trump_corpus[-1]=final_trump_corpus[-1]+\"!\"\n",
    "    elif sentence ==\"?\":  #add separated multiple \"?\" back to the sentence\n",
    "        final_trump_corpus[-1]=final_trump_corpus[-1]+\"?\"\n",
    "    elif numbers.match(sentence[0]): #if sentence starts with number and the last sentence ends with a number and dot then merge\n",
    "        if number_plus_dot.match(final_trump_corpus[-1][-2:]):\n",
    "            final_trump_corpus[-1] = final_trump_corpus[-1] + sentence\n",
    "    else:\n",
    "        final_trump_corpus.append(sentence)\n",
    "\n",
    "#write sentences into file\n",
    "trump_file_output = open(\"..\\\\data\\\\final_trump_corpus.txt\",\"w\",)\n",
    "for sentence in final_trump_corpus:\n",
    "    try:\n",
    "        trump_file_output.write(str(sentence)+\"\\n\")\n",
    "    except UnicodeEncodeError:\n",
    "        continue\n",
    "trump_file_output.close() "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
