{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#---------------------------------------------------------------------\n",
    "#This code tags a corpus with the Stanford tagger\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "import jpype\n",
    "import re\n",
    "import ast\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#Parts of Stanford parser from:\n",
    "#https://github.com/ayushjaiswal/multipass4coreference/tree/master/stanford-parser-python-r22186\n",
    "#\n",
    "#Additionally edited a part and added 'wordTokenPairs' which\n",
    "#creates a tuple output\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "class TextStandoff:\n",
    "    def __init__(self, text, range):\n",
    "        self.entireText = text\n",
    "\n",
    "        self.range = range\n",
    "        \n",
    "    def asPrimitives(self):\n",
    "        return (self.entireText, self.range)\n",
    "    \n",
    "    @staticmethod\n",
    "    def fromPrimitives(args):\n",
    "        return TextStandoff(*args)\n",
    "        \n",
    "    def isNull(self):\n",
    "        return self.range == (0, 0)\n",
    "    \n",
    "    @property\n",
    "    def text(self):\n",
    "        start, end = self.range\n",
    "        return self.entireText[start:end]\n",
    "    @property\n",
    "    def length(self):\n",
    "        start, end = self.range\n",
    "        return end - start\n",
    "    @property\n",
    "    def end(self):\n",
    "        start, end = self.range\n",
    "        return end\n",
    "    @property\n",
    "    def start(self):\n",
    "        start, end = self.range\n",
    "        return start\n",
    "\n",
    "    def overlaps(self, standoff):\n",
    "        if self.start < standoff.end and standoff.start < self.end:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def contains(self, standoff):\n",
    "        start, end = standoff\n",
    "        return self.start <= start and self.end >= end\n",
    "    def before(self, standoff):\n",
    "        if self.end <= standoff.start:\n",
    "            return True\n",
    "        else:\n",
    "            return False\n",
    "    def degreeOfOverlap(self, standoff):\n",
    "        \"\"\"\n",
    "        Returns the size of the overlapping range of two tags. Returns\n",
    "        zero if they do not overlap.\n",
    "        \"\"\"\n",
    "        start, end = standoff\n",
    "        if self.overlaps(standoff):\n",
    "            return min(end, self.end) - max(start, self.start)\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def __iter__(self):\n",
    "        return iter((self.start, self.end))\n",
    "    def toXml(self, standoff):\n",
    "        standoff.setAttribute(\"start\", str(self.start))\n",
    "        standoff.setAttribute(\"end\", str(self.end))\n",
    "\n",
    "    def __repr__(self):\n",
    "        return 'TextStandoff(\"%s\", (%d, %d))' % (self.entireText, self.start, self.end)\n",
    "\n",
    "    def __str__(self):\n",
    "        return '(\"%s\", (%d, %d))' % (self.text, self.start, self.end)\n",
    "\n",
    "    def __eq__(self, obj):\n",
    "        if isinstance(obj, TextStandoff):\n",
    "            if self.range == obj.range and self.entireText == obj.entireText:\n",
    "                return True\n",
    "        return False\n",
    "\n",
    "    def __hash__(self):\n",
    "        return hash(self.entireText) * 17 + hash(self.range)\n",
    "\n",
    "class ParserError(Exception):\n",
    "    def __init__(self, *args, **margs):\n",
    "        Exception.__init__(self, *args,**margs)\n",
    "\n",
    "def standoffFromToken(txt, token):\n",
    "    return TextStandoff(txt, (token.beginPosition(), token.endPosition()))\n",
    "\n",
    "class Dependencies:\n",
    "    def __init__(self, sentence, tokens, posTags, dependencies, wordTokenPairs):\n",
    "        ###\n",
    "        #self.standoffTokens = standoffTokens\n",
    "        ###\n",
    "        self.wordTokenPairs = wordTokenPairs\n",
    "        \n",
    "        self.sentence = sentence\n",
    "\n",
    "        self.posTags = posTags        \n",
    "        \n",
    "        self.tokens = tokens\n",
    "\n",
    "        self.tokensToPosTags = dict(zip(self.tokens, self.posTags))\n",
    "\n",
    "        self.dependencies = dependencies\n",
    "        \n",
    "        self.govToDeps = {}\n",
    "        self.depToGov = {}\n",
    "        self.constituentsToRelation = {}\n",
    "\n",
    "        #there is a bug where sometimes there is a self-dependence\n",
    "        self.dependencies = [(relation, gov, dep) for relation, gov, dep in self.dependencies\n",
    "                             if gov != dep]\n",
    "\n",
    "        for relation, gov, dep in self.dependencies:\n",
    "\n",
    "            self.govToDeps.setdefault(gov, [])\n",
    "            self.govToDeps[gov].append(dep)\n",
    "            #assert not dep in self.depToGov, (dep.text, [(key.text, value.text) for key, value in self.depToGov.iteritems()])\n",
    "            self.depToGov[dep] = gov\n",
    "            self.constituentsToRelation[(gov,dep)] = relation\n",
    "            \n",
    "        self.checkRep()\n",
    "\n",
    "    def tagForTokenStandoff(self, tokenStandoff):\n",
    "        return self.tokensToPosTags[tokenStandoff]\n",
    "        \n",
    "    def checkRep(self):\n",
    "        assert len(self.posTags) == len(self.posTags)        \n",
    "        for t in self.tokens:\n",
    "            assert t.entireText == self.sentence\n",
    "\n",
    "    def govForDep(self, dep):\n",
    "        return self.depToGov[dep]\n",
    "    def depsForGov(self, gov):\n",
    "        return self.govToDeps[gov]\n",
    "\n",
    "    def relForConstituents(self, gov, dep):\n",
    "        return self.constituentsToRelation((gov, dep))\n",
    "    \n",
    "    def __str__(self):\n",
    "        result = \"\"\n",
    "        result += \"sentence=\" + repr(self.sentence) + \"\\n\"\n",
    "        for relation, gov, dep in self.dependencies:\n",
    "            result += relation + \"(\" + gov.text + \", \" + dep.text + \")\\n\"\n",
    "        return result\n",
    "\n",
    "stanford_parser_home = None\n",
    "\n",
    "def startJvm():\n",
    "    import os\n",
    "    os.environ.setdefault(\"STANFORD_PARSER_HOME\", r\"..\\stanford-parser-python-r22186\\3rdParty\\stanford-parser\\stanford-parser-2010-08-20\")\n",
    "    global stanford_parser_home\n",
    "    stanford_parser_home = os.environ[\"STANFORD_PARSER_HOME\"]\n",
    "    jpype.startJVM(jpype.getDefaultJVMPath(),\n",
    "                   \"-ea\",\n",
    "                   \"-Djava.class.path=%s/stanford-parser.jar\" % (stanford_parser_home),)\n",
    "\n",
    "class Parser:\n",
    "    def __init__(self, pcfg_model_fname=None):\n",
    "        if pcfg_model_fname == None:\n",
    "            self.pcfg_model_fname = \"%s/../englishPCFG.July-2010.ser\" % stanford_parser_home            \n",
    "        else:\n",
    "            self.pcfg_model_fname = pcfg_model_fname\n",
    "\n",
    "        self.package_lexparser = jpype.JPackage(\"edu.stanford.nlp.parser.lexparser\")\n",
    "        \n",
    "        self.parser = self.package_lexparser.LexicalizedParser(self.pcfg_model_fname)\n",
    "        self.package = jpype.JPackage(\"edu.stanford.nlp\")\n",
    "\n",
    "        tokenizerFactoryClass = self.package.process.__getattribute__(\"PTBTokenizer$PTBTokenizerFactory\")\n",
    "        self.tokenizerFactory = tokenizerFactoryClass.newPTBTokenizerFactory(True, True)\n",
    "\n",
    "        self.documentPreprocessor = self.package.process.DocumentPreprocessor(self.tokenizerFactory)\n",
    "        \n",
    "        self.parser.setOptionFlags([\"-retainTmpSubcategories\"])\n",
    "\n",
    "    def printInfo(self):\n",
    "\n",
    "        Numberer = self.package.util.Numberer\n",
    "        print (\"Grammar\\t\" +\n",
    "               Numberer.getGlobalNumberer(\"states\").total() + '\\t' +\n",
    "               Numberer.getGlobalNumberer(\"tags\").total() + '\\t' +\n",
    "               Numberer.getGlobalNumberer(\"words\").total() + '\\t' +\n",
    "               self.parser.pparser.ug.numRules() + '\\t' +\n",
    "               self.parser.pparser.bg.numRules() + '\\t' +\n",
    "               self.parser.pparser.lex.numRules())\n",
    "\n",
    "        print(\"ParserPack is \", self.parser.op.tlpParams.getClass())\n",
    "        print(\"Lexicon is \", self.parser.pd.lex.getClass())        \n",
    "        print(\"Tags are: \", Numberer.getGlobalNumberer(\"tags\"))\n",
    "        self.parser.op.display()\n",
    "        print(\"Test parameters\")\n",
    "        self.parser.op.tlpParams.display();\n",
    "        self.package_lexparser.Test.display()\n",
    "    def parse(self, sentence):\n",
    "        \"\"\"\n",
    "        Parses the sentence string, returning the tokens and the parse tree as a tuple.\n",
    "        tokens, tree = parser.parse(sentence)\n",
    "        \"\"\"\n",
    "        tokens = self.documentPreprocessor.getWordsFromString(sentence)\n",
    "\n",
    "        for token in tokens:\n",
    "            if token.word() in [\"down\"]:\n",
    "                token.setTag(\"IN\")\n",
    "                pass\n",
    "            if token.word().lower() in [\"bot\"]:\n",
    "                token.setTag(\"NN\")\n",
    "                pass\n",
    "\n",
    "        wasParsed = self.parser.parse(tokens)\n",
    "        \n",
    "        if not wasParsed:\n",
    "            raise ParserError(\"Could not parse \" + sentence)\n",
    "        return tokens, self.parser.getBestParse()\n",
    "    \n",
    "    def parseToStanfordDependencies(self, sentence):\n",
    "\n",
    "        tokens, tree = self.parse(sentence)\n",
    "        standoffTokens = [standoffFromToken(sentence, token) for token in tokens]\n",
    "        posTags = [token.tag() for token in tree.taggedYield()]\n",
    "        wordTokenPairs = [(word.text, tag) for word, tag in zip(standoffTokens, posTags)] #added for ANLP project\n",
    "        result = self.package.trees.EnglishGrammaticalStructure(tree)\n",
    "        \n",
    "        returnList = []\n",
    "        for dependency in result.typedDependenciesCollapsedTree():\n",
    "\n",
    "            govStandoff = standoffTokens[dependency.gov().index() - 1]\n",
    "            depStandoff = standoffTokens[dependency.dep().index() - 1]\n",
    "\n",
    "            returnList.append((str(dependency.reln()),\n",
    "                               govStandoff,\n",
    "                               depStandoff))\n",
    "\n",
    "        return Dependencies(sentence, standoffTokens, posTags, returnList, wordTokenPairs)\n",
    "\n",
    "startJvm() #one jvm per python instance\n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#Write corpus file into list\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "sentence_corpus = []\n",
    "\n",
    "with open('..\\\\data\\\\example_topic_corpus_edited.txt') as f:\n",
    "    sentence_corpus = f.readlines()\n",
    "sentence_corpus = [x.strip() for x in sentence_corpus] \n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#Write Stanford tagged corpus as file\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "stanford_tagged_sentences = []\n",
    "parser = Parser()\n",
    "\n",
    "file = open('..\\\\data\\\\example_tagged_topic_corpus.txt', 'w', encoding = 'utf-8') \n",
    "\n",
    "file.write('[')\n",
    "for sentence in sentence_corpus:\n",
    "    file.write(str(parser.parseToStanfordDependencies(sentence).wordTokenPairs))\n",
    "    file.write(',' + '\\n')\n",
    "\n",
    "file.write(']')\n",
    "file.close() \n",
    "\n",
    "#---------------------------------------------------------------------\n",
    "#Reconnect @ tuple\n",
    "#---------------------------------------------------------------------\n",
    "\n",
    "file = open('..\\\\data\\\\example_tagged_topic_corpus.txt', 'r', encoding = 'utf-8') \n",
    "tag_list = ast.literal_eval(file.read())\n",
    "file.close()\n",
    "\n",
    "new_tag_list = []\n",
    "\n",
    "for sentence_idx, sentence in enumerate(tag_list):    \n",
    "    new_tag_sentence = []\n",
    "    skip = []\n",
    "    \n",
    "    for tuple_idx, pair in enumerate(sentence):\n",
    "        for word_idx, word in enumerate(pair):\n",
    "            if word_idx == 0:\n",
    "                if tuple_idx not in skip:\n",
    "                    if word==\"@\":\n",
    "                        new_tag_sentence.append((\"@\"+sentence[tuple_idx+1][0], sentence[tuple_idx+1][1]))\n",
    "                        skip.append(tuple_idx+1)\n",
    "                    else:\n",
    "                        new_tag_sentence.append(pair)\n",
    "                    \n",
    "    new_tag_list.append(new_tag_sentence)\n",
    "\n",
    "file = open('..\\\\data\\\\example_tagged_topic_corpus.txt', 'w', encoding = 'utf-8')\n",
    "file.write(str(new_tag_list))\n",
    "file.close()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
